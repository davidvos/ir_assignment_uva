{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"RUNNING THE CELLS BELOW WILL TAKE A LONG TIME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5 points) For each of the LSI models you built over AP, and for LDA, select the\n",
    "\f",
    "ve top signi\f",
    "cant topics from your model. Check the top terms in each topic. Which\n",
    "topics actually represent a particular subject? Analyse the results. Do you observe a\n",
    "di\u000b",
    "erence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "evaluatable": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle as pkl\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import read_ap\n",
    "import download_ap\n",
    "\n",
    "from gensim_corpus import GensimCorpus\n",
    "\n",
    "from trec import TrecAPI\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from lda import LatentDirichletAllocation\n",
    "from lsi import LatentSemanticIndexing\n",
    "\n",
    "from trec import TrecAPI\n",
    "\n",
    "# ensure dataset is downloaded\n",
    "# download_ap.download_dataset()\n",
    "# pre-process the text\n",
    "docs_by_id = None\n",
    "docs_by_id = read_ap.get_processed_docs()\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "os.makedirs(\"saved_models/sim_temps\", exist_ok=True)\n",
    "os.makedirs(\"raw_output\", exist_ok=True)\n",
    "\n",
    "gensim_corpus = GensimCorpus(docs_by_id, embedding=\"bow\")\n",
    "lda = LatentDirichletAllocation(gensim_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda.model.print_topics(num_topics=-1)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_corpus = GensimCorpus(docs_by_id, embedding=\"tfidf\")\n",
    "lsi = LatentSemanticIndexing(gensim_corpus, embedding=\"tfidf\")\n",
    "lsi.model.print_topics(num_topics=-1)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_corpus = GensimCorpus(docs_by_id, embedding=\"bow\")\n",
    "lsi = LatentSemanticIndexing(gensim_corpus, embedding=\"bow\")\n",
    "lsi.model.print_topics(num_topics=-1)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle as pkl\n",
    "import operator\n",
    "from pprint import pprint\n",
    "import argparse\n",
    "\n",
    "model_names = [\n",
    "    \"lsi_bow\",\n",
    "    \"lsi_tfidf\",\n",
    "    \"doc2vec\",\n",
    "    \"doc2vec_vocab_size\",\n",
    "    \"doc2vec_window_size\",\n",
    "    \"doc2vec_vec_dim\",\n",
    "    \"word2vec\",\n",
    "    \"lsa_bow\"\n",
    "]\n",
    "\n",
    "best_run_results = {\n",
    "    \"tfidf\":\"./results/tfidf.json\",\n",
    "    \"word2vec\": \"./results/skip_gram.json\",\n",
    "    \"doc2vec\":\"./results/doc2vec_vocab_size_50000_results_trec.json\",\n",
    "    \"lsi_tfidf\":\"./results/lsi-tfidf-embedding-2000-topics.json\",\n",
    "    \"lsi_bow\":\"./results/lsi-bow-embedding-2000-topics.json\",\n",
    "    \"lda_bow\":\"./results/lda-500-topics.json\"\n",
    "}\n",
    "\n",
    "default_run_results = {\n",
    "    \"tfidf\":\"./results/tfidf.json\",\n",
    "    \"word2vec\": \"./results/skip_gram.json\",\n",
    "    \"doc2vec\":\"./results/doc2vec_vocab_size_10000_results_trec.json\",\n",
    "    \"lsi_tfidf\":\"./results/lsi-tfidf-embedding-500-topics.json\",\n",
    "    \"lsi_bow\":\"./results/lsi-bow-embedding-500-topics.json\",\n",
    "    \"lda_bow\":\"./results/lda-500-topics.json\"\n",
    "}\n",
    "\n",
    "lsi_bow_results = {\n",
    "    \"10 topics\":\"./results/lsi-bow-embedding-10-topics.json\",\n",
    "    \"50 topics\":\"./results/lsi-bow-embedding-50-topics.json\",\n",
    "    \"100 topics\":\"./results/lsi-bow-embedding-100-topics.json\",\n",
    "    \"500 topics\":\"./results/lsi-bow-embedding-500-topics.json\",\n",
    "    \"1000 topics\":\"./results/lsi-bow-embedding-1000-topics.json\",\n",
    "    \"2000 topics\":\"./results/lsi-bow-embedding-2000-topics.json\",\n",
    "}\n",
    "\n",
    "lsi_tfidf_results = {\n",
    "    \"10 topics\":\"./results/lsi-tfidf-embedding-10-topics.json\",\n",
    "    \"50 topics\":\"./results/lsi-tfidf-embedding-50-topics.json\",\n",
    "    \"100 topics\":\"./results/lsi-tfidf-embedding-100-topics.json\",\n",
    "    \"500 topics\":\"./results/lsi-tfidf-embedding-500-topics.json\",\n",
    "    \"1000 topics\":\"./results/lsi-tfidf-embedding-1000-topics.json\",\n",
    "    \"2000 topics\":\"./results/lsi-tfidf-embedding-2000-topics.json\",\n",
    "}\n",
    "\n",
    "doc2vec_window_size_results = {\n",
    "    \"window size 5\":\"./results/doc2vec_window_size_5_results_trec.json\",\n",
    "    \"window size 10\":\"./results/doc2vec_window_size_10_results_trec.json\",\n",
    "    \"window size 15\":\"./results/doc2vec_window_size_15_results_trec.json\",\n",
    "    \"window size 20\":\"./results/doc2vec_window_size_20_results_trec.json\"\n",
    "}\n",
    "\n",
    "doc2vec_vec_dim_results = {\n",
    "    \"vec dim 200\":\"./results/doc2vec_vec_dim_200_results_trec.json\",\n",
    "    \"vec dim 300\":\"./results/doc2vec_vec_dim_300_results_trec.json\",\n",
    "    \"vec dim 400\":\"./results/doc2vec_vec_dim_400_results_trec.json\",\n",
    "    \"vec dim 500\":\"./results/doc2vec_vec_dim_500_results_trec.json\"\n",
    "}\n",
    "\n",
    "doc2vec_vocab_size_results = {\n",
    "    \"vocab size 1000\":\"./results/doc2vec_vocab_size_10000_results_trec.json\",\n",
    "    \"vocab size 25000\":\"./results/doc2vec_vocab_size_25000_results_trec.json\",\n",
    "    \"vocab size 50000\":\"./results/doc2vec_vocab_size_50000_results_trec.json\",\n",
    "    \"vocab size 100000\":\"./results/doc2vec_vocab_size_100000_results_trec.json\",\n",
    "    \"vocab size 200000\":\"./results/doc2vec_vocab_size_200000_results_trec.json\"\n",
    "}\n",
    "\n",
    "doc2vec_results = dict(\n",
    "    list(doc2vec_window_size_results.items()) + \n",
    "    list(doc2vec_vec_dim_results.items()) + \n",
    "    list(doc2vec_vocab_size_results.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AQ 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5 points) Report the retrieval performance in terms of MAP and nDCG for all\n",
    "of the methods, on a) all queries, and b) queries 76-100. To be precise, you need to report\n",
    "24 numbers in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n############# AQ4.1 #############\\n\\n\")\n",
    "default_all_results_per_setup = {}\n",
    "default_results_per_setup = {}\n",
    "default_eval_results_per_setup = {}\n",
    "for model, fn in default_run_results.items():\n",
    "    with open(fn, \"r\") as f:\n",
    "        res = json.load(f)\n",
    "        res.pop(\"all\", None)\n",
    "        default_all_results_per_setup[model] = res\n",
    "\n",
    "    default_results_per_setup[model] = {}\n",
    "    default_results_per_setup[model][\"map\"] = sum([q[\"map\"] for q in res.values()]) / len(res)\n",
    "    default_results_per_setup[model][\"ndcg\"] = sum([q[\"ndcg\"] for q in res.values()]) / len(res)\n",
    "\n",
    "    # query ids range from 51 to 200. evaluation set is 76 - 100, we should only use that for\n",
    "    # parameter tuning.\n",
    "    query_id_range = list(str(qid) for qid in range(76, 101)) \n",
    "    default_eval_results_per_setup[model] = {}\n",
    "    default_eval_results_per_setup[model][\"map\"] = sum([res[qid][\"map\"] for qid in query_id_range]) / len(query_id_range)\n",
    "    default_eval_results_per_setup[model][\"ndcg\"] = sum([res[qid][\"ndcg\"] for qid in query_id_range]) / len(query_id_range)        \n",
    "\n",
    "print(\"RETRIEVAL PERFORMANCE ON ALL QUERIES:\")\n",
    "pprint(default_results_per_setup)\n",
    "\n",
    "print(\"\\nRETRIEVAL PERFORMANCE ON EVAL QUERIES 76-100\")\n",
    "pprint(default_eval_results_per_setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AQ 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5 points) Signi\f",
    "cance testing is a way of showing that if the di\u000b",
    "erence observed\n",
    "between the performance of two models is due to chance or not. Use pytrec eval to\n",
    "perform a t-test between every pair of models, on the complete query set. You need to\n",
    "report the results of six tests.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "for model_1, model_2 in combinations(list(default_all_results_per_setup.keys()), 2):\n",
    "    \n",
    "    first_results = default_all_results_per_setup[model_1]\n",
    "    second_results = default_all_results_per_setup[model_2]\n",
    "    \n",
    "    query_ids = list(\n",
    "    set(first_results.keys()) & set(second_results.keys()))\n",
    "\n",
    "    first_scores = [\n",
    "        first_results[query_id][\"map\"] for query_id in query_ids]\n",
    "    second_scores = [\n",
    "        second_results[query_id][\"map\"] for query_id in query_ids]\n",
    "\n",
    "    print(\"For {} and {}: {}\".format(model_1, model_2, ttest_rel(first_scores, second_scores)[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AQ 4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5 points) Report the retrieval performance in terms of MAP and nDCG for all\n",
    "of the methods, on a) all queries, and b) test queries. Use the parameters leading to the\n",
    "best performance on validation set. Attach the results corresponding to each run to\n",
    "your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the model type to determine the best settings for.\n",
    "model = \"lsi_bow\"\n",
    "\n",
    "# query ids range from 51 to 200. evaluation set is 76 - 100, we should only use that for\n",
    "# parameter tuning.\n",
    "query_id_range = list(str(qid) for qid in range(76, 101))\n",
    "\n",
    "results_per_setup = {}\n",
    "param_results = {}\n",
    "for setup, fn in eval(model + \"_results\").items():\n",
    "    with open(fn, \"r\") as f:\n",
    "        all_results = json.load(f)\n",
    "        param_results[setup] = all_results\n",
    "            \n",
    "    results_per_setup[setup] = sum([all_results[qid][\"map\"] for qid in query_id_range]) / len(query_id_range)\n",
    "    #results_per_setup[fn] = all_results[\"all\"][\"map\"]\n",
    "\n",
    "setup, mean_eval_map = max(results_per_setup.items(), key=operator.itemgetter(1))\n",
    "\n",
    "print(\"######### TO DETERMINE THE BEST SETUP ###########\\n\\n\")\n",
    "print(\"ALL RESULTS:\")\n",
    "pprint(results_per_setup)\n",
    "print(\"\\nBEST RESULT:\\nWith MAP of: {0:.4f} best setup: {1}\".format(mean_eval_map, setup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_per_setup = {}\n",
    "results_per_setup = {}\n",
    "eval_results_per_setup = {}\n",
    "for model, fn in best_run_results.items():\n",
    "    with open(fn, \"r\") as f:\n",
    "        res = json.load(f)\n",
    "        res.pop(\"all\", None)\n",
    "        all_results_per_setup[model] = res\n",
    "\n",
    "    results_per_setup[model] = {}\n",
    "    results_per_setup[model][\"map\"] = sum([q[\"map\"] for q in res.values()]) / len(res)\n",
    "    results_per_setup[model][\"ndcg\"] = sum([q[\"ndcg\"] for q in res.values()]) / len(res)\n",
    "\n",
    "    # query ids range from 51 to 200. evaluation set is 76 - 100, we should only use that for\n",
    "    # parameter tuning.\n",
    "    query_id_range = list(str(qid) for qid in range(76, 101)) \n",
    "    eval_results_per_setup[model] = {}\n",
    "    eval_results_per_setup[model][\"map\"] = sum([res[qid][\"map\"] for qid in query_id_range]) / len(query_id_range)\n",
    "    eval_results_per_setup[model][\"ndcg\"] = sum([res[qid][\"ndcg\"] for qid in query_id_range]) / len(query_id_range)\n",
    "\n",
    "print(\"RETRIEVAL PERFORMANCE ON ALL QUERIES:\")\n",
    "pprint(default_results_per_setup)\n",
    "\n",
    "print(\"\\nRETRIEVAL PERFORMANCE ON EVAL QUERIES 76-100\")\n",
    "pprint(default_eval_results_per_setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AQ 4.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AQ4.4: (5 points) Perform a t-test between the result of each method using the best\n",
    "parameters and the default ones used in AQ4.1. Describe your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "for model in default_all_results_per_setup:\n",
    "    \n",
    "    first_results = default_all_results_per_setup[model]\n",
    "    second_results = all_results_per_setup[model]\n",
    "    \n",
    "    query_ids = list(\n",
    "    set(first_results.keys()) & set(second_results.keys()))\n",
    "\n",
    "    first_scores = [\n",
    "        first_results[query_id][\"map\"] for query_id in query_ids]\n",
    "    second_scores = [\n",
    "        second_results[query_id][\"map\"] for query_id in query_ids]\n",
    "\n",
    "    print(\"For {}: {}\".format(model, ttest_rel(first_scores, second_scores)[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AQ 4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AQ4.5: (5 points)For each parameter, plot the retrieval performance in terms of MAP\n",
    "with respect to the value of the parameter, for a) all queries, and b) test queries. Describe\n",
    "your \f",
    "ndings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "tune_models = [\n",
    "    \"lsi_bow\", \"lsi_tfidf\", \"doc2vec_vocab_size\", \"doc2vec_window_size\", \"doc2vec_vec_dim\"\n",
    "]\n",
    "\n",
    "query_id_range = list(str(qid) for qid in range(76, 101))\n",
    "\n",
    "for model in tune_models:\n",
    "    results_per_setup = {}\n",
    "    eval_results_per_setup = {}\n",
    "    for setup, fn in eval(model + \"_results\").items():\n",
    "        with open(fn, \"r\") as f:\n",
    "            res = json.load(f)\n",
    "\n",
    "        results_per_setup[setup] = sum([q[\"map\"] for q in res.values()]) / len(res)\n",
    "        eval_results_per_setup[setup] = sum([res[qid][\"map\"] for qid in query_id_range]) / len(query_id_range)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 6))\n",
    "    setups = list(results_per_setup.keys())\n",
    "    maps = list(results_per_setup.values())\n",
    "    ax[0].bar(setups, maps, color=\"orange\", label=\"MAP on full results\")\n",
    "    ax[0].set_title(model)\n",
    "\n",
    "    setups = list(eval_results_per_setup.keys())\n",
    "    maps = list(results_per_setup.values())\n",
    "    ax[1].bar(setups, maps, label=\"MAP on eval results\")\n",
    "    ax[1].set_title(model)\n",
    "    plt.legend()\n",
    "    plt.savefig(\"./results/param-tuning-plots-{}.png\".format(model))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AQ 4.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AQ4.6: (5 points) Doing a query-level analysis provides insights on the weaknesses and\n",
    "strengths of the retrieval models. For each of the four retrieval methods implemented\n",
    "above, find success and failure cases: queries for which the MAP was highest or lowest.\n",
    "Analyse the results, possibly with checking the qrels file. Discuss why you think each case\n",
    "happened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrels, queries = read_ap.read_qrels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_per_model = {}\n",
    "\n",
    "for model, fn in best_run_results.items():\n",
    "    with open(fn, \"r\") as f:\n",
    "        res = json.load(f)\n",
    "        res.pop(\"all\", None)\n",
    "        res = {qid:res[qid][\"map\"] for qid in res.keys()}\n",
    "\n",
    "    res_per_model[model] = {\n",
    "        \"best\":max(res.items(), key=operator.itemgetter(1)),\n",
    "        \"worst\":min(res.items(), key=operator.itemgetter(1)),\n",
    "    }\n",
    "\n",
    "for model, stats in res_per_model.items():\n",
    "    best = stats[\"best\"][0]\n",
    "    worst = stats[\"worst\"][0]\n",
    "    \n",
    "    print(\"\\n#######{}########\".format(model))\n",
    "    print(\"With MAP of {}, BEST query {}:\\n\".format(stats[\"best\"][1], best))\n",
    "    print(queries[best])\n",
    "    print(\"relevant docs:\\n\", qrels[best].keys(), len(qrels[best].keys()))\n",
    "    print(\"\\nWith MAP of {}, WORST query {}:\\n\".format(stats[\"worst\"][1], worst))\n",
    "    print(queries[worst])\n",
    "    print(\"relevant docs:\\n\", qrels[worst].keys(), len(qrels[worst].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AQ 4.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AQ4.7: (5 points) Find the top-5 queries that have the highest variance in terms of MAP\n",
    "between different retrieval models. Provide an analysis why the performance of the models\n",
    "differs a lot on these queries compared to the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "map_per_qid = defaultdict(lambda:{})\n",
    "\n",
    "for model, fn in best_run_results.items():\n",
    "    with open(fn, \"r\") as f:\n",
    "        res = json.load(f)\n",
    "        res.pop(\"all\", None)\n",
    "        res = {qid:res[qid][\"map\"] for qid in res.keys()}\n",
    "\n",
    "    for qid, val in res.items():\n",
    "        map_per_qid[qid][model] = val\n",
    "\n",
    "var_per_qid = {}\n",
    "        \n",
    "for qid, vals in map_per_qid.items():\n",
    "    var_per_qid[qid] = np.var(list(vals.values()))\n",
    "    \n",
    "sorted_vars = [(k, v) for k, v in sorted(var_per_qid.items(), key=lambda item: -item[1])]\n",
    "highest_var = sorted_vars[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q, var in highest_var:\n",
    "    print(\"Variance: {} Query - {}: {}\".format(var, q, queries[q]))\n",
    "    \n",
    "    for model, val in map_per_qid[q].items():\n",
    "        print(\"MAP for model: {} - {}\".format(model, val))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
